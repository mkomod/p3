\documentclass[12pt]{article}
\input{settings.tex}
\input{shorthand.tex}

\author{Michael Komodromos}
\title{Projects}


\begin{document}
   
\section{Mutual Information CCA}

\subsection{Definitions}

\textbf{Definition:} Entropy of a random variable $X \sim p_X$,
\begin{equation}
H(X) = - \E_X \left[\log p_X (x) \right]
\end{equation}

\textbf{Definition:} Mutual information of random variables $X \sim p_X$ and $Y \sim p_Y$ with joint density $p_{XY}$,
\begin{equation}
    I(X, Y) = \E_{XY} \left[ \log \left( \frac{p_{XY}(x, y)}{p_X(x) p_Y(y)} \right) \right]
\end{equation}

It follows that the mutual information can be written in terms of the entropy,
\begin{align}
    I(X, Y)
    =&\ H(Y) + H(X) - H(X, Y) \\
    =&\ H(X) - H(X | Y) \\
    =&\ H(Y) - H(Y | X)
\end{align}
where the conditional entropy is defined as $H(Y|X) = - \E_{XY} [ \log p_{X | Y}(x | y)]$

Which in turn provide some nice interpretations, for instance when stated as $H(X) - H(X | Y)$ the mutual information is the reduction of uncertainty in $X$ given knowledge of $Y$ \citep{Cover2006}

\textbf{Definition:} Kullback-Leibler divergence for density functions $p_X$ and $q_X$,
\begin{equation}
\KL(p_X \| q_X) = \E_{p_X} \left[ \log \frac{p_X(x)}{q_X(x)} \right]
\end{equation}

\subsection{CCA}

Canonical correlation analysis involves finding linear combinations of two random variables such that the resulting linear combinations are maximally correlated.

Let $X = (X_1, \dots, X_p)^\top \sim p_X$ and $Y = (Y_1, \dots, Y_q)^\top \sim p_Y$ for $p, q \geq 1$, then the \textit{canonical correlation vectors} are given by solving,
\begin{equation} \label{eq:cca}
    a^\ast, b^\ast = \underset{a, b}{\argmax} \corr \left(a^\top X, b^\top Y \right)
\end{equation}
where $a \in \R^p$ and $b \in \R^q$.


\subsection{Mutual Information CCA}


Mutual information CCA (ICCA) introduced by \cite{Yin2004} generalizes CCA by using the mutual information (rather than the correlation) as a measure of dependence. Formally, the \textit{informational canonical correlation vectors} (or \textit{informational vectors}) are given by,
\begin{equation} \label{eq:icca}
    a^\ast, b^\ast = \underset{a, b}{\argmax}\ I \left(a^\top X, b^\top Y \right)
\end{equation}

\subsection{Relationship with CCA}

Fortunately, there is a nice connection between ICCA and CCA. Suppose
\begin{equation}
    \begin{pmatrix} X \\ Y \end{pmatrix}
    \sim N \left(
    \begin{pmatrix} \mu_X \\ \mu_Y \end{pmatrix},
    \begin{pmatrix} \Sigma_{X} & \Sigma_{XY} \\ \Sigma_{YX} & \Sigma_Y \end{pmatrix}
    \right)
\end{equation}
Then, the mutual information between $a^\top X$ and $b^\top Y$ is given as
\begin{equation}
    I(a^\top X, b^\top Y) = -\frac{1}{2} \log \left(1 - \corr^2(a^\top X, b^\top Y) \right)
\end{equation}
Given the monotonicity of the logarithm, finding the informational vectors can be reduced to optimizing over $ \corr^2 (a^\top X, b^\top Y) $. Therefore solving \eqref{eq:icca} where $X, Y$ are Gaussian is equivalent to solving \eqref{eq:cca}.


\subsection{Related methods}




\subsection{Practical implementation}

Similar to the correlation matrix, the mutual information must be estimated. Denote the observed data as $\D = \{ (x_i, y_i)_{i=1}^n \} $ where $ n $ are the number of observations and $x_i \in \R^p$, $y_i \in \R^q$, then an estimator for the mutual information is given by,
\begin{equation}
    \widehat{I}(a^\top X, b^\top Y) = \frac{1}{n} \sum_{i=1}^{n} \log \frac{\widehat{p}_{XY}(a^\top x_i, b^\top y_i)}{\widehat{p}_X(a^\top x_i)\widehat{p}_Y(b^\top y_i)}
\end{equation}
where $\widehat{p}_{XY}$ is an estimated density for the joint of $a^\top X, b^\top Y$ and $\widehat{p}_X, \widehat{p}_Y$ is an estimated density for the marginals of $X, Y$ respectively.

Estimating the joint and marginal densities can be challenging. For instance, one method could involve kernel density estimators (KDEs), i.e. an estimator of the form
\begin{equation}
\widehat{p_Z}(z) = \frac{1}{nh} \sum_{i=1}^{n} K \left( \frac{z - z_i}{h} \right)
\end{equation}
where $h>0$ is the bandwidth, typically taken as $h=1.06 \sigma_z n^{-1/5}$ where $\sigma_Z$ is the standard deviation of $Z$ and $ z_i \overset{\text{iid.}}{\sim} Z $.


\subsection{Estimators}

Other methods beyond naive KDEs have been used to estimate the mutual information. Typically these methods build on estimators of the entropy.

\subsubsection{Naive kNN Estimator}
\newcommand{\Hknn}{\widehat{H}_{\text{kNN}}}

Given by
\begin{equation}
\Hknn (x) = - \frac{1}{n} \sum_{i=1}^{n} \log \widehat{p}_k \left( x_i \right) - (\psi(k) - \log(k))
\end{equation}
where 
\begin{equation}
\widehat{p}_k(x_i) = \frac{k}{n - 1}\frac{\Gamma (p / 2 + 1)}{\pi^{p /2}} r_k (x_i)^{-p}
\end{equation}
and $\psi(\cdot)$ is the digamma function and $r_k(x_i)$ is the Euclidean distance from $x_i$ and the $k$th nearest neighbour.


\subsection{Extensions}

\subsubsection{Regularized ICCA}

A natural extension to ICCA is regularizing the objective \eqref{eq:icca}, i.e. solving
\begin{equation}
a^\ast, b^\ast = \underset{a, b}{\argmax}\ I \left(a^\top X, b^\top Y \right) - g(a) - h(b)
\end{equation}
where $g, h$ are regularizes of $a, b$ e.g. $g(a; \lambda_a) = \lambda_a |a|$.

\subsubsection{More than two views}

A further extension is to more than two views. Denote the different views by $X_k, k=1,\dots,K$, then instead of the mutual information we use the so called \textit{interaction information}, defined as,
\begin{equation}
    I(X_1, \dots, X_K) = I(X_1, \dots, X_{K-1}) - I(X_1, \dots, X_{K-1} | X_K)
\end{equation}
where $ I(X_1, \dots, X_{K-1} | X_K) = \E_{X_K} \left[ I(X_1, \dots, X_{K-1}) | X_K \right]$. 

\textit{Example with K=3.} 
\begin{align}
    I(X_1, X_2, X_3) =&\ I(X_1, X_2) - I(X_1, X_2 | X_3) \\
    = &\ \E_{X_1 X_2} \left[ \log \frac{p_{X_1 X_2}}{p_{X_1} p_{X_2}} \right] - 
    \E_{X_3} \left\{
	\E_{X_1 X_2| X_3} \left[
	    \log \frac{p_{X_1 X_2 | X_3}}{p_{X_1|X_3} p_{X_2|X_3}}
	\right]
    \right\} \\
    = &\ \E_{X_1 X_2} \left[ \log \frac{p_{X_1 X_2}}{p_{X_1} p_{X_2}} \right] - 
    \E_{X_1 X_2 X_3} \left[
	    \log \frac{p_{X_1 X_3 X_3} p_{X_3}}{p_{X_1 X_3} p_{X_2 X_3}}
    \right] \\
    = &\ \E_{X_1 X_2 X_3} \left[
	\log \frac{p_{X_1 X_2} p_{X_1 X_3} p_{X_2 X_3}}{p_{X_1 X_2 X_3} p_{X_1} p_{X_2} p_{X_3}}
    \right]
\end{align}

Extending \eqref{eq:icca} to $K$ views could involve solving:
\begin{equation}
    \{ a_k^\ast \}_{k=1}^K = \underset{a_k, k=1,\dots,K}{\argmax} I(a_1^\top X_1, a_2^\top X_2, \dots, a_K^\top X_K)
\end{equation}

An alternate generalization is via the \textit{total correlation} defined as,
\begin{equation}
    C(X_1, \dots, X_K) = \KL ( p_{X_1, \dots, X_K} \| p_{X_1} \times \cdots \times p_{X_K} )
\end{equation}
which can be written in terms of the entropies,
\begin{equation}
    C(X_1, \dots, X_K) = \left( \sum_{k=1}^{K} H(X_K) \right) - H(X_1, \dots, X_K)
\end{equation}
where $H (X) = - \E_X \left[\log p_X(x) \right]$.

\newpage
\section{Bayesian CCA}

First a detour

\subsection{Probabilistic PCA}

PCA can be expressed as the maximum likelihood solution of a probabilistic latent variable model \citep{Tipping1999, Bishop2006}. There are several advantages under the probabilistic framework:
\begin{itemize}
    \item Computationally efficient EM algorithm can be used when we are only interested in the leading eigenvectors
    \item EM formulation allows for dealing with missing data
    \item Leads to the Bayesian treatment in which the dimensionality can be found in a principled way
    \item Can be run generatively to provide samples from the distribution
\end{itemize}
The model is outlined as follows:
\begin{equation} \label{eq:ppca} 
\begin{aligned}
    X | Z \overset{\text{ind}}{\sim} &\ N(WZ + \mu, \sigma^2 I_p) \\
    Z \overset{\text{iid}}{\sim} &\ N(0, I_k)
\end{aligned}
\end{equation}
where $W \in \R^{p \times k}$ and $I_p$ is the $p \times p$ identity matrix. Which in turn, can be written as
\begin{equation} \label{eq:ppca_gen} 
    X = WZ + \mu + \epsilon, \quad \epsilon \overset{\text{iid}}{\sim} N(0, \sigma^2 I_p)
\end{equation}
where $ Z \overset{\text{iid}}{\sim} N(0, I_k) $. It follows that $X \sim N(\mu, C)$ where $C = WW^\top + \sigma^2I_p$, 

Estimating the parameters $W, \mu$ and $\sigma$ can be performed via maximum likelihood. Denoting the observed data as $\X = \{x_1, \dots, x_n \}$ for $x_i \in \R^p$, the log likelihood function is given by
\begin{align}
    \ell (\X | \mu, W, \sigma) = &\ \sum_{i=1}^{n} \log p(x_n | \mu, W, \sigma) 
    \nonumber \\
    = &\ \sum_{i=1}^{n} - \frac{p}{2} \log(2 \pi) - \frac{1}{2} \log | C | 
    - \frac{1}{2}(x_i - \mu)^\top C^{-1}(x_i-\mu) 
    \nonumber \\
    = &\ - \frac{np}{2} \log(2 \pi) - \frac{n}{2} \log | C | 
    - \frac{1}{2} \sum_{i=1}^{n} \tr \left\{ C^{-1}(x_i-\mu)(x_i - \mu)^\top \right\}
    \nonumber \\
    = &\ -\frac{n}{2} \left[ p \log(2 \pi) + \log | C | 
    + \tr \left\{ C^{-1} S \right\} \right]
    \label{eq:ppca_mle}
\end{align}
The maximum likelihood estimate of $\mu$ is given by $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$. The maximum likelihood estimate for $W$ is then given by differentiating \eqref{eq:ppca_mle} wrt $W$, setting to zero and rearranging, recalling that $C = WW^\top + \sigma^2 I_p$. The derivation requires the application of the chain rule for matrices \eqref{eq:matrix_chain_rule}, and follows as,
\begin{align}
    \frac{\partial \ell}{\partial W_{ij}} = &\
    \tr \left\{ \left( \frac{\partial \ell}{\partial C} \right)^\top 
	\frac{\partial C}{\partial W_{ij}} \right\}  \nonumber \\
    = &\ - \frac{n}{2} \tr \left\{
	\left( \frac{\partial}{\partial C} \log | C | \right)^\top 
	\frac{\partial C}{\partial W_{ij}}
	+ \left( \frac{\partial}{\partial C} \tr  \left\{ C^{-1} S \right\} \right)^\top 
	\frac{\partial C}{\partial W_{ij}}
    \right\} \nonumber \\
    = &\ - \frac{n}{2} \tr \left\{
	C^{-1}          \frac{\partial C}{\partial W_{ij}}
      - C^{-1}SC^{-1}   \frac{\partial C}{\partial W_{ij}}
    \right\} \nonumber
\end{align}
where the derivatives with respect to $C$ are standard identities \citep[eq. (57), eq. (63)]{MCB}. Furthermore,
\begin{equation}
\frac{\partial C}{\partial W_{ij}} = \frac{\partial}{\partial W_{ij}} \left[
    WW^\top + \sigma^2 I_p \right] 
\end{equation}
which is an $n \times n$ matrix where the $i$th row and $i$th column is given by $(w_{1j}, w_{2j}, \dots,\\ w_{i-1, j}, 2 w_{ij}, w_{i+1,j}, \dots, w_{n, j})$ and is $0$ elsewhere.
Using the fact that for a symmetric matrix $U \in \R^{n \times n}$,
\begin{equation}
    \tr \left\{ U \frac{\partial C}{\partial W_{ij}} \right\} 
    = 2 U_{i:} \left( \frac{\partial C}{\partial W_{ij}} \right)_{:j}
\end{equation}
where the notation $X_{i:}$ is the $i$th row and $X_{:j}$ the $j$th column of $X$,
and noting $C^{-1}$ and $C^{-1}SC^{-1}$ are symmetric we have,
\begin{equation}\label{eq:ppca_mle_w}
    \frac{\partial \ell}{\partial W} = nC^{-1}SC^{-1} W - nC^{-1} W 
\end{equation}
Setting \eqref{eq:ppca_mle_w} to zero and rearranging gives
\begin{equation}
W = SC^{-1}W
\end{equation}


\newpage
\section{Group spike-and-slab regression}

Consider the model
\begin{equation}
    y = X\beta + \epsilon
\end{equation}
where $\beta = (\beta_1, \dots, \beta_p)^\top \in \R^p$, $X \in \R^{n \times p}$ and $\epsilon$ is a noise term. Further, let $G_k = \{ G_{k_1}, \dots, G_{k_p} \}$ for $k=1,\dots,M$ be disjoint sets of indices, such that $ \bigcup_{k=1}^M G_k = \{1, \dots, p \}$.

Based on the above notations, there are several sparsity patterns that $\beta$ can take:
\begin{itemize}
    \itemsep1pt
    \item \textbf{Coordinate sparsity}: wherein few coordinate of $\beta$ are nonzero.
    \item \textbf{Group sparsity}: where few vectors $\beta_{G_k} = (\beta_{G_{k_1}}, \dots, \beta_{G_{k_p}})$ are non-zero.
    \item \textbf{Sparse-group sparsity}: where few vectors $\beta_{G_k} = (\beta_{G_{k_1}}, \dots, \beta_{G_{k_p}})$ are non-zero and the vectors themselves are sparse.
\end{itemize}




\subsection{Patterns of sparsity}


\subsection{}
Considering the regression model
\begin{equation}
y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n
\end{equation}
where $y \in \R$, $x \in \R^p$. $f(x)$. 
Sparse linear regression takes the form
\begin{equation}
    f(x) = \langle \beta, x \rangle, \quad \beta \in \R^p
\end{equation}
In turn many representations of $f(x)$ can be re-cast into the sparse linear regression framework, for instance:
\begin{itemize}
    \item Sparse piecewise consent regression, e.g. analysis of cop number variations along the DNA chain.
    \begin{equation}
	f(x) = \sum_{j \in S} c_j \I_{x \geq z_j},\quad x \in \R
    \end{equation}
    \item Sparse basis expansion, e.g. de-noising, representing cortex signals.
    \begin{equation}
	f(x) = \sum_{j \in S} c_j \phi_j(x), \quad x \in \R
    \end{equation}
    \item Sparse additive model with group-sparse regression
    \begin{equation}
	f(x) = \sum_{k=1}^{p} \sum_{j \in J}^{} c_{j, k} \phi_j (x_k)
    \end{equation}
\end{itemize}





\subsection{LASSO}

\citep{Giraud2021}


\newpage
\bibliography{refs.bib}

\appendix
% \renewcwection.\arabic{equation}}
\numberwithin{equation}{section}

\section{Matrix Derivatives}

Let $g: U \rightarrow \R$ and $X \in \R^{n \times p}$ then,
\begin{equation} \label{eq:matrix_chain_rule}
    \frac{\partial g(U)}{\partial X_{ij}} = \tr \left( 
	\left[ \frac{\partial g(U)}{\partial U} \right]^\top
	 \frac{\partial U}{\partial X_{ij}} \right)
\end{equation}
See \citep[Sec 2.8.1]{MCB}.




\end{document}
