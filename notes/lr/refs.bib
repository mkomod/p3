@article{Yin2004,
    author = {Yin, Xiangrong},
    doi = {10.1016/S0047-259X(03)00129-5},
    issn = {0047259X},
    journal = {Journal of Multivariate Analysis},
    keywords = {Canonical correlation analysis,Multivariate analysis,Mutual information,Permutation test},
    number = {2},
    pages = {161--176},
    title = {{Canonical correlation analysis based on information theory}},
    volume = {91},
    year = {2004}
}
@article{Gao2015,
    abstract = {We demonstrate that a popular class of non-parametric mutual information (MI) estimators based on k-nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on local uniformity of the underlying joint distribution. We introduce a new estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data.},
    archivePrefix = {arXiv},
    arxivId = {1411.2003},
    author = {Gao, Shuyang and {Ver Steeg}, Greg and Galstyan, Aram},
    eprint = {1411.2003},
    issn = {15337928},
    journal = {Journal of Machine Learning Research},
    pages = {277--286},
    title = {{Efficient estimation of mutual information for strongly dependent variables}},
    volume = {38},
    year = {2015}
}
@book{Cover2006,
  author = {Cover, Thomas M. and Thomas, Joy A.},
  isbn = {0471241954},
  month = {July},
  publisher = {Wiley-Interscience},
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)},
  year = {2006}
}
@book{Bishop2006,
  author = {Christopher M. Bishop},
  title = {Pattern Recognition and Machine Learning},
  publisher = {Springer},
  year = {2006}
}
@article{Tipping1999,
    abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
    author = {Tipping, Michael E. and Bishop, Christopher M.},
    doi = {10.1111/1467-9868.00196},
    issn = {13697412},
    journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
    keywords = {Density estimation,EM algorithm,Gaussian mixtures,Maximum likelihood,Principal component analysis,Probability model},
    number = {3},
    pages = {611--622},
    title = {{Probabilistic principal component analysis}},
    volume = {61},
    year = {1999}
}
@misc{MCB,
    author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
    title = {{The Matrix Cookbook}},
    url = {https://www.math.uwaterloo.ca/{~}hwolkowi/matrixcookbook.pdf},
    year = {2012}
}
@book{Giraud2021,
  doi = {10.1201/9781003158745},
  url = {https://doi.org/10.1201/9781003158745},
  year = {2021},
  month = aug,
  publisher = {Chapman and Hall/{CRC}},
  author = {Christophe Giraud},
  title = {Introduction to High-Dimensional Statistics}
}
